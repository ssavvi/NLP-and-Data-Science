"""This file handles building the keras model for the pipeline. 
The embedding layer needs to be created so that the model can use it. 
Afterwards, the model can be created with the preprocessor.

`prepare_embedding_layer` must be run first.
"""
import numpy as np

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding, Input, Conv1D, MaxPooling1D 
from tensorflow.keras.layers import Flatten, Dense, LSTM, SpatialDropout1D, Dropout
from tensorflow.keras.models import Model, Sequential

from sklearn.base import BaseEstimator, TransformerMixin

import spacy


# These values are updated inside the `prepare_embedding_layer`
# function, but are shared with the other functions, hence they
# are declared outside the scope of the function and the `global`
# keyword is used.
TOKENIZER = None
EMBEDDING_LAYER = None
MAX_SEQUENCE_LENGTH = None
def prepare_embedding_layer(
    documents,
    nlp_model='en_core_sci_lg',
    max_tokenizer_words=15000,
    max_sequence_length=None
):
    """Prepares a shared embedding layer for the Keras models 
    generated by `create_model`.
    
    `documents` are the document strings to fit the tokenizer
    to. The max length of these documents once tokenized is 
    saved as the shared maximum length, unless a different one
    is specified.  
    `nlp_model` is the spacy model used to generate the initial 
    word vectors.
    """
    global TOKENIZER
    global EMBEDDING_LAYER
    global MAX_SEQUENCE_LENGTH
    
    # create tokenizer to build vocabulary
    TOKENIZER = Tokenizer(num_words=max_tokenizer_words)
    TOKENIZER.fit_on_texts(documents)
    
    if max_sequence_length is None:
        sequences = TOKENIZER.texts_to_sequences(documents)
        max_sequence_length = max(map(len, sequences))
    MAX_SEQUENCE_LENGTH = max_sequence_length
    
    # create embedding matrix
    nlp = spacy.load(nlp_model)
    embedding_dimension = len(nlp.vocab.get_vector('horse'))  # random word to get shape
    
    embeddings_index = {}
    for word in TOKENIZER.word_index.keys():
        try:
            embeddings_index[word] = nlp.vocab.get_vector(word)
        except KeyError as e:
            print(f"Didn't work for word: {word}")
    
    embedding_matrix = np.zeros((len(TOKENIZER.word_index) + 1, embedding_dimension))
    for word, i in TOKENIZER.word_index.items():
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            # words not found in embedding index will be all-zeros.
            embedding_matrix[i] = embedding_vector
    
    # EMBEDDING_LAYER can't be trainable because it's shared
    # across different k-folds in the cross-validation. Repeatedly
    # training it would bias the cross-validation.
    EMBEDDING_LAYER = Embedding(
        len(TOKENIZER.word_index) + 1,
        embedding_dimension,
        weights=[embedding_matrix],
        input_length=max_sequence_length,
        trainable=False
    )  
    return True

def create_model(
    output_dim,
    max_sequence_length=None,
    optimizer='adam',
    conv_kernel_size=8,
    conv_pool_steps=2,
    **model_params
):
    assert EMBEDDING_LAYER is not None, \
            "Run `prepare_embedding_layer` first."
    if max_sequence_length is None:
        max_sequence_length = MAX_SEQUENCE_LENGTH
    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')
    x = EMBEDDING_LAYER(sequence_input)
    for i in range(conv_pool_steps):
        x = Conv1D(32, conv_kernel_size, activation='relu')(x)
        x = Conv1D(32, conv_kernel_size, activation='relu')(x)
        x = MaxPooling1D(pool_size=2)(x)    
    x = Flatten()(x)
    x = Dropout (0.2)
    x = Dense(150, activation='relu')(x)
    preds = Dense(output_dim, activation='softmax')(x)

    model = Model(sequence_input, preds)
    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer=optimizer,
                  metrics=['acc'])
    return model


def create_model_lstm(
    output_dim,
    lstm_dim=100,
    optimizer='adam'
):
    assert EMBEDDING_LAYER is not None, \
            "Run `prepare_embedding_layer` first."
    model = Sequential()
    model.add(EMBEDDING_LAYER)
    model.add(SpatialDropout1D(0.2))
    model.add(LSTM(lstm_dim, dropout=0.2))
    model.add(LSTM(lstm_dim, dropout=0.2))
    model.add(Dense(output_dim, activation="softmax"))
    model.compile(loss="sparse_categorical_crossentropy",
                  optimizer="adam",
                  metrics=["acc"])
    return model


# TODO: implement custom tokenizer instead of default Keras one
class TokenizeAndPadTransformer(BaseEstimator, TransformerMixin):
    """Inspired by https://medium.com/@diegoglozano/building-a-pipeline-for-nlp-b569d51db2d1
    
    Uses the default Keras tokenizer. This class preprocesses text 
    for the Keras model in the sklearn pipeline. 
    Wraps the Tokenizer created with `prepare_embedding_layer`.
    """
    def __init__(self, max_sequence_length=None):
        assert TOKENIZER is not None, \
            "Run `prepare_embedding_layer` first."
        
        self.tokenizer = TOKENIZER
        if max_sequence_length is None:
            max_sequence_length = MAX_SEQUENCE_LENGTH
        self.max_sequence_length = max_sequence_length
        
    def fit(self, X, y=None):
        # Tokenizer is already fit at this point
        return self
    
    def transform(self, X, y=None):
        # Tokenizes texts to sequences, then pads them to fixed length
        X_sequences = self.tokenizer.texts_to_sequences(X)
        X_padded = pad_sequences(X_sequences, maxlen=self.max_sequence_length)
        return X_padded
